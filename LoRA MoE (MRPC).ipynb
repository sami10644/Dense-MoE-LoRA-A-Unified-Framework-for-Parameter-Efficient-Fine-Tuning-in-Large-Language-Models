{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8947e2c-a06e-4f07-a676-f1ea41f1565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification, RobertaLayer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8060a85-69ee-4460-b093-df4f0227671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "#import the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "raw_datasets  = load_dataset(\"glue\", 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d944887-3b8c-4d0d-a9a8-539c3d3960bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "\n",
    "# Load the configuration of the pre-trained model\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.hidden_dropout_prob=0.0\n",
    "config.attention_probs_dropout_prob=0.00\n",
    "# Load the tokenizer for the pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c9cec-2d91-415f-8f88-a28a3d73773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "col_to_delete = ['sentence1','sentence2']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True,max_length=512)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649b2e5-3661-4188-b455-34f086ae5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     LoRAExpert defines a lightweight adapter module that uses low-rank \n",
    "#     approximation for efficient fine-tuning of neural networks.\n",
    "\n",
    "class LoRAExpert(nn.Module):\n",
    "    def __init__(self, input_size, output_size, rank):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Linear(input_size, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the first layer (A) and then the second layer (B)\n",
    "        return self.lora_B(self.lora_A(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ece96-1387-486a-9221-09bdf49f9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class SparseMoeConfig(PretrainedConfig):\n",
    "    model_type = \"sparse_moe\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50265,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        is_decoder=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=1,\n",
    "        classifier_dropout=None,\n",
    "        # DenseMoE-specific parameters\n",
    "        num_local_experts=8,       # Number of local experts\n",
    "        num_experts_per_tok=1,\n",
    "        top_k=8,                   # Number of top-k experts activated\n",
    "        router_jitter_noise=0.01,  # Jitter noise for router logits\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_act = hidden_act\n",
    "        self.top_k = top_k  \n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.is_decoder = is_decoder\n",
    "        self.use_cache = use_cache\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.pad_token_id = pad_token_id\n",
    "        # SparseMoE-specific attributes\n",
    "        self.num_local_experts = num_local_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.router_jitter_noise = router_jitter_noise\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2396c-750c-41c0-b2b7-0cc76bee77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SparseMoeBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.top_k = config.top_k\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([\n",
    "            LoRAExpert(self.hidden_dim, self.hidden_dim, rank=4) for _ in range(self.num_experts)\n",
    "        ])  # List of experts\n",
    "        self.aux_loss_weight = 0.05  # Weight for auxiliary loss\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        batch_size, seq_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states_flat = hidden_states.view(-1, hidden_dim)  # Flatten the input for processing\n",
    "\n",
    "        # Compute routing logits and routing weights\n",
    "        router_logits = self.gate(hidden_states_flat)\n",
    "        routing_weights, selected_experts = torch.topk(F.softmax(router_logits, dim=-1), self.top_k, dim=-1)\n",
    "\n",
    "        # Auxiliary loss: Balance routing probabilities\n",
    "        expert_mask = F.one_hot(selected_experts, num_classes=self.num_experts).sum(dim=1)\n",
    "        fraction_tokens = expert_mask.float().sum(dim=0) / hidden_states_flat.size(0)\n",
    "        average_probs = (expert_mask.float() * routing_weights).sum(dim=0) / hidden_states_flat.size(0)\n",
    "        auxiliary_loss = self.aux_loss_weight * self.num_experts * torch.sum(fraction_tokens * average_probs)\n",
    "\n",
    "        # Forward pass for selected experts\n",
    "        final_hidden_states = torch.zeros_like(hidden_states_flat)\n",
    "        for expert_idx in range(self.num_experts):\n",
    "        # Get tokens routed to this expert\n",
    "            token_idx = torch.where(expert_mask[:, expert_idx] > 0)[0]   # Get tokens for the expert\n",
    "            token_states = hidden_states_flat[token_idx]\n",
    "            \n",
    "            # Compute expert output\n",
    "            expert_output = self.experts[expert_idx](token_states)\n",
    "            expert_output *= routing_weights[token_idx, expert_idx].unsqueeze(-1)\n",
    "        \n",
    "            # Accumulate expert contributions\n",
    "            final_hidden_states.index_add_(0, token_idx, expert_output)\n",
    "\n",
    "        # Reshape output and return with auxiliary loss\n",
    "        return final_hidden_states.view(batch_size, seq_length, hidden_dim), auxiliary_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94debb1f-e64b-483f-861d-a006b54f6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import RobertaLayer\n",
    "\n",
    "class RobertaLayerWithMoE(RobertaLayer):\n",
    "    def __init__(self, config, moe_config):\n",
    "        super().__init__(config)\n",
    "        self.moe_block = SparseMoeBlock(moe_config) # Initialize SparseMoeBlock with its configuration ( though we implement DenseMoE )\n",
    "        self.use_moe = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        use_cache=False,\n",
    "    ):\n",
    "        # Compute attention outputs\n",
    "        attention_output = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )[0]\n",
    "\n",
    "        if self.use_moe:   # Apply MoE block if enabled\n",
    "            moe_output, auxiliary_loss = self.moe_block(attention_output)\n",
    "            ffn_output = self.intermediate(moe_output)\n",
    "            layer_output = self.output(ffn_output, moe_output)\n",
    "            return layer_output, auxiliary_loss\n",
    "        else:   # Default transformer behavior\n",
    "            intermediate_output = self.intermediate(attention_output)  # Process through intermediate FFN\n",
    "            layer_output = self.output(intermediate_output, attention_output)  # Combine with output layer\n",
    "            return layer_output, 0.0  # Return output and zero auxiliary loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06ad90-79bf-4f95-bbc9-6d44f27378ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "class RobertaForSequenceClassificationWithMoE(RobertaForSequenceClassification):\n",
    "    def __init__(self, config, moe_config):\n",
    "        super().__init__(config)\n",
    "        # Replace the encoder layers with MoE-enabled layers for the last 4 layers\n",
    "        self.roberta.encoder.layer = nn.ModuleList([\n",
    "            RobertaLayerWithMoE(config, moe_config) if i >= config.num_hidden_layers - 4 else RobertaLayer(config)\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        # Forward pass through the RoBERTa model\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Extract hidden states and auxiliary losses from MoE layers\n",
    "        hidden_states = outputs.last_hidden_state  # Retrieve the final hidden states\n",
    "        auxiliary_losses = [layer[1] for layer in outputs if isinstance(layer, tuple)]  # Collect auxiliary losses\n",
    "        total_auxiliary_loss = sum(auxiliary_losses) if auxiliary_losses else 0.0  # Sum auxiliary losses\n",
    "        # Pass hidden states through the classification head\n",
    "        logits = self.classifier(hidden_states)\n",
    "        # Compute the combined loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            main_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))  # Compute main loss\n",
    "            loss = main_loss + total_auxiliary_loss  # Add auxiliary loss to main loss\n",
    "        # Return loss and logits if loss is computed, otherwise just return logits\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cec848-f12c-405f-9f7a-83fe8b75979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for our Dense MoE LoRA\n",
    "\n",
    "moe_config = SparseMoeConfig(\n",
    "    vocab_size=50265,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,  # Match the pretrained model\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=514,  # Match RoBERTa\n",
    "    type_vocab_size=1,  # Match RoBERTa\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    num_local_experts=8,\n",
    "    num_experts_per_tok=8,\n",
    "    router_jitter_noise=0.05,\n",
    "    top_k=8\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import RobertaModel\n",
    "import torch.nn.init as init\n",
    "\n",
    "base_model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the custom model\n",
    "custom_model = RobertaForSequenceClassificationWithMoE(config=base_model.config, moe_config=moe_config)\n",
    "\n",
    "# Manually load the pretrained weights into the custom model\n",
    "custom_model.load_state_dict(base_model.state_dict(), strict=False)\n",
    "\n",
    "# Initialize MoE-Specific Parameters\n",
    "def initialize_moe_params(moe_block):\n",
    "    # Initialize gate weights\n",
    "    init.xavier_uniform_(moe_block.gate.weight)\n",
    "    # Initialize LoRA expert weights\n",
    "    for expert in moe_block.experts:\n",
    "        if isinstance(expert, LoRAExpert):  # Check for LoRAExpert instance\n",
    "            init.xavier_uniform_(expert.lora_A.weight)\n",
    "            init.xavier_uniform_(expert.lora_B.weight)\n",
    "        else:\n",
    "            # Handle cases where the expert is not a LoRAExpert\n",
    "            for layer in expert:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        init.zeros_(layer.bias)\n",
    "\n",
    "# Apply initialization to MoE blocks\n",
    "for layer in custom_model.roberta.encoder.layer:\n",
    "    if hasattr(layer, \"moe_block\"):\n",
    "        initialize_moe_params(layer.moe_block)\n",
    "\n",
    "\n",
    "num_moe_layers = 4  # Enable MoE for the last 4 layers\n",
    "for i, layer in enumerate(custom_model.roberta.encoder.layer):\n",
    "    if i < len(custom_model.roberta.encoder.layer) - num_moe_layers:\n",
    "        layer.use_moe = False # Disable MoE for earlier layers\n",
    "    else:\n",
    "        layer.use_moe = True  # Enable MoE for later layers\n",
    "# Freeze base model parameters\n",
    "for param in custom_model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze LoRA parameters\n",
    "for name, param in custom_model.named_parameters():\n",
    "    if \"lora_\" in name:  # LoRA-specific parameters\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze MoE parameters\n",
    "\n",
    "for layer in custom_model.roberta.encoder.layer:\n",
    "    if hasattr(layer, \"moe_block\"):\n",
    "        # Unfreeze gate weights and LoRA expert parameters\n",
    "        for name, param in layer.moe_block.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0f8d1d-2b84-477d-b5a0-4f757d49f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da7fdb7-64f7-4b1f-ace1-59b59035042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='qnli_dir',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.00,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # choosing from: 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=custom_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62ff1f5-60ed-4e61-b5b6-71158a37b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 03:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.711618</td>\n",
       "      <td>0.158088</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.316176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.610477</td>\n",
       "      <td>0.341912</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.406114</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.503438</td>\n",
       "      <td>0.770513</td>\n",
       "      <td>0.552763</td>\n",
       "      <td>0.514597</td>\n",
       "      <td>0.713235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.388823</td>\n",
       "      <td>0.803140</td>\n",
       "      <td>0.813828</td>\n",
       "      <td>0.807945</td>\n",
       "      <td>0.830882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>0.340140</td>\n",
       "      <td>0.828327</td>\n",
       "      <td>0.836209</td>\n",
       "      <td>0.832016</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>0.325915</td>\n",
       "      <td>0.843922</td>\n",
       "      <td>0.826998</td>\n",
       "      <td>0.834528</td>\n",
       "      <td>0.860294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.330173</td>\n",
       "      <td>0.849883</td>\n",
       "      <td>0.832666</td>\n",
       "      <td>0.840334</td>\n",
       "      <td>0.865196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=770, training_loss=0.4840301191651976, metrics={'train_runtime': 219.2298, 'train_samples_per_second': 167.313, 'train_steps_per_second': 3.512, 'total_flos': 1562840021046720.0, 'train_loss': 0.4840301191651976, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
