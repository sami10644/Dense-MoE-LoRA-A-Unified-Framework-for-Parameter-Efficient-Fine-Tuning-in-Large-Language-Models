{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d794b77c-2f40-4e41-926d-4746fc7029a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
    "#from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training, AdaLoraConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification, RobertaLayer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7671d9-9281-4faf-b4cb-a44a4341c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "raw_datasets  = load_dataset(\"glue\", 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0be92d-2263-415d-a78e-d653b393cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "# Load the configuration of the pre-trained model\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.hidden_dropout_prob=0.0\n",
    "config.attention_probs_dropout_prob=0.00\n",
    "# Load the tokenizer for the pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7472ee5a-8a68-4f30-9a3e-9511e4926c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "col_to_delete = ['idx', 'sentence']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e002aec-de9d-4ea6-9268-c8de7b57f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     LoRAExpert defines a lightweight adapter module that uses low-rank \n",
    "#     approximation for efficient fine-tuning of neural networks.\n",
    "   \n",
    "\n",
    "class LoRAExpert(nn.Module):\n",
    "    def __init__(self, input_size, output_size, rank):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Linear(input_size, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the first layer (A) and then the second layer (B)\n",
    "        return self.lora_B(self.lora_A(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6de660-2561-48a2-a000-44758492c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class SparseMoeConfig(PretrainedConfig):\n",
    "    model_type = \"sparse_moe\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50265,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        is_decoder=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=1,\n",
    "        classifier_dropout=None,\n",
    "        # DenseMoE-specific parameters\n",
    "        num_local_experts=8,  # Number of local experts\n",
    "        num_experts_per_tok=1,\n",
    "        top_k=8,         # Number of top-k experts activated\n",
    "        router_jitter_noise=0.01,  # Jitter noise for router logits\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_act = hidden_act\n",
    "        self.top_k = top_k  \n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.is_decoder = is_decoder\n",
    "        self.use_cache = use_cache\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.pad_token_id = pad_token_id\n",
    "        # SparseMoE-specific attributes\n",
    "        self.num_local_experts = num_local_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.router_jitter_noise = router_jitter_noise\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8358876-23b7-4fed-9ae6-57fd6372fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SparseMoeBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.top_k = config.top_k\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([\n",
    "            LoRAExpert(self.hidden_dim, self.hidden_dim, rank=4) for _ in range(self.num_experts)\n",
    "        ])  # List of experts\n",
    "        self.aux_loss_weight = 0.05  # Weight for auxiliary loss\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        batch_size, seq_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states_flat = hidden_states.view(-1, hidden_dim)   # Flatten the input for processing\n",
    "\n",
    "        # Compute routing logits and routing weights\n",
    "        router_logits = self.gate(hidden_states_flat)\n",
    "        routing_weights, selected_experts = torch.topk(F.softmax(router_logits, dim=-1), self.top_k, dim=-1)\n",
    "\n",
    "        # Auxiliary loss: Balance routing probabilities\n",
    "        expert_mask = F.one_hot(selected_experts, num_classes=self.num_experts).sum(dim=1)\n",
    "        fraction_tokens = expert_mask.float().sum(dim=0) / hidden_states_flat.size(0)\n",
    "        average_probs = (expert_mask.float() * routing_weights).sum(dim=0) / hidden_states_flat.size(0)\n",
    "        auxiliary_loss = self.aux_loss_weight * self.num_experts * torch.sum(fraction_tokens * average_probs)\n",
    "\n",
    "        # Forward pass for selected experts\n",
    "        final_hidden_states = torch.zeros_like(hidden_states_flat)\n",
    "        for expert_idx in range(self.num_experts):\n",
    "        # Get tokens routed to this expert\n",
    "            token_idx = torch.where(expert_mask[:, expert_idx] > 0)[0]   # Get tokens for the expert\n",
    "            token_states = hidden_states_flat[token_idx]\n",
    "            \n",
    "            # Compute expert output\n",
    "            expert_output = self.experts[expert_idx](token_states)\n",
    "            expert_output *= routing_weights[token_idx, expert_idx].unsqueeze(-1)\n",
    "        \n",
    "            # Accumulate expert contributions\n",
    "            final_hidden_states.index_add_(0, token_idx, expert_output)\n",
    "\n",
    "        # Reshape output and return with auxiliary loss\n",
    "        return final_hidden_states.view(batch_size, seq_length, hidden_dim), auxiliary_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad68f12-9c57-492c-987c-9af79e989529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import RobertaLayer\n",
    "\n",
    "class RobertaLayerWithMoE(RobertaLayer):\n",
    "    def __init__(self, config, moe_config):\n",
    "        super().__init__(config)\n",
    "        self.moe_block = SparseMoeBlock(moe_config)   # Initialize SparseMoeBlock with its configuration (though we implement DenseMoE )\n",
    "        self.use_moe = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        use_cache=False,\n",
    "    ):\n",
    "        # Compute attention outputs\n",
    "        attention_output = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )[0]\n",
    "\n",
    "        if self.use_moe:   # Apply MoE block if enabled\n",
    "            moe_output, auxiliary_loss = self.moe_block(attention_output)\n",
    "            ffn_output = self.intermediate(moe_output)\n",
    "            layer_output = self.output(ffn_output, moe_output)\n",
    "            return layer_output, auxiliary_loss\n",
    "        else:   # Default transformer behavior\n",
    "            intermediate_output = self.intermediate(attention_output)  # Process through intermediate FFN\n",
    "            layer_output = self.output(intermediate_output, attention_output)  # Combine with output layer\n",
    "            return layer_output, 0.0  # Return output and zero auxiliary loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b74d0c-f6da-41ca-9f15-1a8f7cecc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "class RobertaForSequenceClassificationWithMoE(RobertaForSequenceClassification):\n",
    "    def __init__(self, config, moe_config):\n",
    "        super().__init__(config)\n",
    "        # Replace the encoder layers with MoE-enabled layers for the last 4 layers\n",
    "        self.roberta.encoder.layer = nn.ModuleList([\n",
    "            RobertaLayerWithMoE(config, moe_config) if i >= config.num_hidden_layers - 4 else RobertaLayer(config)\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        # Forward pass through the RoBERTa model\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Extract hidden states and auxiliary losses from MoE layers\n",
    "        hidden_states = outputs.last_hidden_state  # Retrieve the final hidden states\n",
    "        auxiliary_losses = [layer[1] for layer in outputs if isinstance(layer, tuple)]  # Collect auxiliary losses\n",
    "        total_auxiliary_loss = sum(auxiliary_losses) if auxiliary_losses else 0.0  # Sum auxiliary losses\n",
    "        # Pass hidden states through the classification head\n",
    "        logits = self.classifier(hidden_states)\n",
    "        # Compute the combined loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            main_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))  # Compute main loss\n",
    "            loss = main_loss + total_auxiliary_loss  # Add auxiliary loss to main loss\n",
    "        # Return loss and logits if loss is computed, otherwise just return logits\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d5108f3-6505-4870-9484-6e74b68090d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Configuration for our Dense MoE LoRA\n",
    "\n",
    "moe_config = SparseMoeConfig(\n",
    "    vocab_size=50265,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,  # Match the pretrained model\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=514,  # Match RoBERTa\n",
    "    type_vocab_size=1,  # Match RoBERTa\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    num_local_experts=8,\n",
    "    num_experts_per_tok=1,\n",
    "    router_jitter_noise=0.05,\n",
    "    top_k=8\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import RobertaModel\n",
    "import torch.nn.init as init\n",
    "\n",
    "base_model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the custom model\n",
    "custom_model = RobertaForSequenceClassificationWithMoE(config=base_model.config, moe_config=moe_config)\n",
    "\n",
    "# Manually load the pretrained weights into the custom model\n",
    "custom_model.load_state_dict(base_model.state_dict(), strict=False)\n",
    "\n",
    "# Initialize MoE-Specific Parameters\n",
    "def initialize_moe_params(moe_block):\n",
    "    # Initialize gate weights\n",
    "    init.xavier_uniform_(moe_block.gate.weight)\n",
    "    # Initialize LoRA expert weights\n",
    "    for expert in moe_block.experts:\n",
    "        if isinstance(expert, LoRAExpert):  # Check for LoRAExpert instance\n",
    "            init.xavier_uniform_(expert.lora_A.weight)\n",
    "            init.xavier_uniform_(expert.lora_B.weight)\n",
    "        else:\n",
    "            # Handle cases where the expert is not a LoRAExpert\n",
    "            for layer in expert:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        init.zeros_(layer.bias)\n",
    "\n",
    "# Apply initialization to MoE blocks\n",
    "for layer in custom_model.roberta.encoder.layer:\n",
    "    if hasattr(layer, \"moe_block\"):\n",
    "        initialize_moe_params(layer.moe_block)\n",
    "\n",
    "\n",
    "num_moe_layers = 4  # Enable MoE for the last 4 layers\n",
    "for i, layer in enumerate(custom_model.roberta.encoder.layer):\n",
    "    if i < len(custom_model.roberta.encoder.layer) - num_moe_layers:\n",
    "        layer.use_moe = False # Disable MoE for earlier layers\n",
    "    else:\n",
    "        layer.use_moe = True  # Enable MoE for later layers\n",
    "# Freeze base model parameters\n",
    "for param in custom_model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze LoRA parameters\n",
    "for name, param in custom_model.named_parameters():\n",
    "    if \"lora_\" in name:  # LoRA-specific parameters\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze MoE parameters\n",
    "\n",
    "for layer in custom_model.roberta.encoder.layer:\n",
    "    if hasattr(layer, \"moe_block\"):\n",
    "        # Unfreeze gate weights and LoRA expert parameters\n",
    "        for name, param in layer.moe_block.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fae2082-4fa8-460f-85cb-e6f167c6c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caa515cd-0978-4a77-977e-626b12f4072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='loramoesparse',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.00,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # We can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=custom_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b68112d-5624-4c30-8aa7-a30749a9c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 813,314\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(custom_model):\n",
    "    return sum(p.numel() for p in custom_model.parameters() if p.requires_grad)\n",
    "\n",
    "# Display trainable parameters\n",
    "print(f\"Number of trainable parameters: {count_trainable_parameters(custom_model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ff53720-a98c-402b-96a3-71d1d428c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1404' max='1404' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1404/1404 11:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.695300</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>0.254587</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.337386</td>\n",
       "      <td>0.509174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.490498</td>\n",
       "      <td>0.785132</td>\n",
       "      <td>0.780005</td>\n",
       "      <td>0.777910</td>\n",
       "      <td>0.778670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.337031</td>\n",
       "      <td>0.867805</td>\n",
       "      <td>0.863833</td>\n",
       "      <td>0.864158</td>\n",
       "      <td>0.864679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.888628</td>\n",
       "      <td>0.887156</td>\n",
       "      <td>0.887423</td>\n",
       "      <td>0.887615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.276373</td>\n",
       "      <td>0.888085</td>\n",
       "      <td>0.886124</td>\n",
       "      <td>0.885234</td>\n",
       "      <td>0.885321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.251877</td>\n",
       "      <td>0.903637</td>\n",
       "      <td>0.903637</td>\n",
       "      <td>0.903637</td>\n",
       "      <td>0.903670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.239842</td>\n",
       "      <td>0.919108</td>\n",
       "      <td>0.918277</td>\n",
       "      <td>0.918488</td>\n",
       "      <td>0.918578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.251329</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>0.902795</td>\n",
       "      <td>0.903271</td>\n",
       "      <td>0.903670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.246140</td>\n",
       "      <td>0.920291</td>\n",
       "      <td>0.918024</td>\n",
       "      <td>0.918398</td>\n",
       "      <td>0.918578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.245509</td>\n",
       "      <td>0.919265</td>\n",
       "      <td>0.916856</td>\n",
       "      <td>0.917239</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.225421</td>\n",
       "      <td>0.918528</td>\n",
       "      <td>0.918614</td>\n",
       "      <td>0.918560</td>\n",
       "      <td>0.918578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.224139</td>\n",
       "      <td>0.926954</td>\n",
       "      <td>0.926370</td>\n",
       "      <td>0.926540</td>\n",
       "      <td>0.926606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.293900</td>\n",
       "      <td>0.222097</td>\n",
       "      <td>0.927943</td>\n",
       "      <td>0.927581</td>\n",
       "      <td>0.927702</td>\n",
       "      <td>0.927752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>0.221015</td>\n",
       "      <td>0.930155</td>\n",
       "      <td>0.929917</td>\n",
       "      <td>0.930005</td>\n",
       "      <td>0.930046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/s/sami20/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1404, training_loss=0.38400899540325173, metrics={'train_runtime': 690.1852, 'train_samples_per_second': 195.162, 'train_steps_per_second': 2.034, 'total_flos': 3304446813017292.0, 'train_loss': 0.38400899540325173, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
